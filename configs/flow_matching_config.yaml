trainer:
  resume: false
  checkpoint: null
  flow_vae_checkpoint: "output/flow_vae/checkpoint_100000.pth"
  gpt_checkpoint: "checkpoints/xtts2/gpt.pth"
  vq_vae_checkpoint: "checkpoints/xtts2/dvae.pth"
  mel_norm_file: "checkpoints/xtts2/mel_stats.pth"

  min_lr: 1e-6
  learning_rate: 2e-4
  weight_decay: 0.01
  grad_clip: 1.0
  epochs: 200
  log_interval: 100
  val_interval: 1000
  save_interval: 5000
  output_dir: "output/flow_matching"

dataset:
  train_file_list: "data/libritts/libritts_train.txt"
  valid_file_list: "data/libritts/libritts_valid.txt"
  tokenizer_file: "checkpoints/xtts2/vocab.json"
  sample_rate: 22050
  num_workers: 16
  batch_size: 32

  min_conditioning_duration: 3.0 # 3s
  max_conditioning_duration: 6.0 # 6s
  min_audio_duration: 0.5
  max_audio_duration: 15.0
  min_text_length: 1.0
  max_text_length: 300

  use_masking_gt_prompt_approach: true

model:
  flow_vae:
    encoder_dim: 64
    encoder_rates: [2, 4, 8, 8]
    latent_dim: 256
    decoder_dim: 1536
    decoder_rates: [8, 8, 4, 2]
    sample_rate: 22050

    n_flows: 4
    flow_dim: 256
    flow_hidden_dim: 256
    flow_kernel_size: 5
    flow_dilation_rate: 1
    flow_layers: 4
    flow_mean_only: true

  vq_vae:
    channels: 80
    normalization: null
    positional_dims: 1
    num_tokens: 1024
    codebook_dim: 512
    hidden_dim: 512
    num_resnet_blocks: 3
    kernel_size: 3
    num_layers: 2
    use_transposed_convs: false

  gpt:
    layers: 30
    model_dim: 1024
    start_text_token: 261 # https://github.com/coqui-ai/TTS/tts/models/xtts.py#L222
    stop_text_token: 0
    heads: 16
    max_text_tokens: 402
    max_mel_tokens: 605
    max_prompt_tokens: 70
    number_text_tokens: 6681
    num_audio_tokens: 1026
    start_audio_token: 1024
    stop_audio_token: 1025
    use_perceiver_resampler: true
    code_stride_len: 1024
  
  flow_matching: # https://github.com/FunAudioLLM/CosyVoice/blob/main/examples/libritts/cosyvoice/conf/cosyvoice.yaml
    input_size: 512 # vq token embedding size
    output_size: 256 # vq token embedding proj & speaker embd output size
    spk_embed_dim: 1024 # speaker embd input size
    vocab_size: 4096
    input_frame_rate: 50 # unused
    only_mask_loss: True
    encoder:
      input_size: 512 # vq token encoder input size
      output_size: 512 # vq token encoder output size
      attention_heads: 8
      linear_units: 2048
      num_blocks: 6
      dropout_rate: 0.1
      positional_dropout_rate: 0.1
      attention_dropout_rate: 0.1
      normalize_before: True
      input_layer: 'linear'
      pos_enc_layer_type: 'rel_pos_espnet'
      selfattention_layer_type: 'rel_selfattn'
      use_cnn_module: False
      macaron_style: False
    length_regulator:
      channels: 256
      sampling_ratios: [1, 1, 1, 1]
    decoder:
      in_channels: 240 # unused
      n_spks: 1 # unused
      spk_emb_dim: 1024 # unused
      cfm_params:
        sigma_min: 1e-06
        solver: 'euler'
        t_scheduler: 'cosine'
        training_cfg_rate: 0.2
        inference_cfg_rate: 0.7
        reg_loss_type: 'l1'
      estimator:
        in_channels: 1024 # concat(vq_token_embedding, flow_vae_latent, spk_embed, cond)
        out_channels: 256
        channels: [256, 256]
        dropout: 0.0
        attention_head_dim: 64
        n_blocks: 4
        num_mid_blocks: 12
        num_heads: 8
        act_fn: 'gelu'
